{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis of IMDB Dataset\n",
    "\n",
    "#### The dataset consists of 50000 movie reviews labelled as positive or negative. I have used Multinomial NB and SVMs and Logistic Regression and VADER to model this sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    25000\n",
       "positive    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "NO_SPACE = \"\"\n",
    "SPACE = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(df)):\n",
    "    df['review'][i] = df['review'][i].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, line.lower()) for line in df['review']]\n",
    "reviews = [REPLACE_WITH_SPACE.sub(SPACE, line) for line in df['review']]\n",
    "reviews = [line.lower() for line in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"one of the other reviewers has mentioned that after watching just 1 oz episode you'll be hooked. they are right, as this is exactly what happened with me. the first thing that struck me about oz was its brutality and unflinching scenes of violence, which set in right from the word go. trust me, this is not a show for the faint hearted or timid. this show pulls no punches with regards to drugs, sex or violence. its is hardcore, in the classic use of the word. it is called oz as that is the nickname given to the oswald maximum security state penitentary. it focuses mainly on emerald city, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. em city is home to many..aryans, muslims, gangstas, latinos, christians, italians, irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away. i would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. forget pretty pictures painted for mainstream audiences, forget charm, forget romance...oz doesn't mess around. the first episode i ever saw struck me as so nasty it was surreal, i couldn't say i was ready for it, but as i watched more, i developed a taste for oz, and got accustomed to the high levels of graphic violence. not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) watching oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['reviews'] = reviews\n",
    "df['reviews'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('review', inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>a wonderful little production.  the filming te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                            reviews\n",
       "0  positive  one of the other reviewers has mentioned that ...\n",
       "1  positive  a wonderful little production.  the filming te...\n",
       "2  positive  i thought this was a wonderful way to spend ti...\n",
       "3  negative  basically there's a family where a little boy ...\n",
       "4  positive  petter mattei's \"love in the time of money\" is..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 blanks:  []\n"
     ]
    }
   ],
   "source": [
    "blanks = []\n",
    "\n",
    "for i,sentiment,rv in df.itertuples():\n",
    "    if type(rv)==str:  \n",
    "        if rv.isspace():\n",
    "            blanks.append(i)\n",
    "        \n",
    "print(len(blanks), 'blanks: ', blanks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['reviews']\n",
    "y = df['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a pipeline to perform TF-IDF Vectorizing and Modelling of Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "text_clf_nb = Pipeline([('tfidf', TfidfVectorizer()),('clf', MultinomialNB()),])\n",
    "text_clf_lsvc = Pipeline([('tfidf', TfidfVectorizer()),('clf', LinearSVC()),])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial NB model and its results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_nb.fit(X_train, y_train)\n",
    "predictions = text_clf_nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7280  928]\n",
      " [1394 6898]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.89      0.86      8208\n",
      "    positive       0.88      0.83      0.86      8292\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     16500\n",
      "   macro avg       0.86      0.86      0.86     16500\n",
      "weighted avg       0.86      0.86      0.86     16500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8592727272727273\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVC Model and its results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_lsvc.fit(X_train, y_train)\n",
    "predictions1 = text_clf_lsvc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7289  919]\n",
      " [ 736 7556]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_test,predictions1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.89      0.90      8208\n",
      "    positive       0.89      0.91      0.90      8292\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     16500\n",
      "   macro avg       0.90      0.90      0.90     16500\n",
      "weighted avg       0.90      0.90      0.90     16500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,predictions1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8996969696969697\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test,predictions1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding selected stopwords and performing the same models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['a', 'about', 'an', 'and', 'are', 'as', 'at', 'be', 'been', 'but', 'by', 'can', 'even', 'ever', 'for', 'from', 'get', 'had', 'has', 'have', 'he', 'her', 'hers', 'his', \\\n",
    "             'how', 'i', 'if', 'in', 'into', 'is', 'it', 'its', 'just', 'me', 'my', 'of', 'on', 'or', 'see', 'seen', 'she', 'so', 'than', 'that', 'the', 'their', 'there', 'they', 'this','to', 'was', 'we', 'were', 'what', 'when', 'which', 'who', 'will', 'with', 'you']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_nb2 = Pipeline([('tfidf', TfidfVectorizer(stop_words=stopwords)),('clf', MultinomialNB()),])\n",
    "text_clf_lsvc2 = Pipeline([('tfidf', TfidfVectorizer(stop_words=stopwords)),('clf', LinearSVC()),])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial NB model with stopwords and its results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7255  953]\n",
      " [1295 6997]]\n"
     ]
    }
   ],
   "source": [
    "text_clf_nb2.fit(X_train, y_train)\n",
    "predictions2 = text_clf_nb2.predict(X_test)\n",
    "print(metrics.confusion_matrix(y_test,predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.88      0.87      8208\n",
      "    positive       0.88      0.84      0.86      8292\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     16500\n",
      "   macro avg       0.86      0.86      0.86     16500\n",
      "weighted avg       0.86      0.86      0.86     16500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8637575757575757\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test,predictions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVC Model with stopwords and its results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7287  921]\n",
      " [ 755 7537]]\n"
     ]
    }
   ],
   "source": [
    "text_clf_lsvc2.fit(X_train, y_train)\n",
    "predictions3 = text_clf_lsvc2.predict(X_test)\n",
    "print(metrics.confusion_matrix(y_test,predictions3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.89      0.90      8208\n",
      "    positive       0.89      0.91      0.90      8292\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     16500\n",
      "   macro avg       0.90      0.90      0.90     16500\n",
      "weighted avg       0.90      0.90      0.90     16500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,predictions3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8984242424242425\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test,predictions3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using countvectorizer and performing Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(X_train)\n",
    "X_train1 = cv.transform(X_train)\n",
    "X_test1 = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression for various values of C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matric for for C = 0.01: \n",
      " [[7178 1030]\n",
      " [ 852 7440]]\n",
      "\n",
      "\n",
      "Classification report for C = 0.01: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.87      0.88      8208\n",
      "    positive       0.88      0.90      0.89      8292\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     16500\n",
      "   macro avg       0.89      0.89      0.89     16500\n",
      "weighted avg       0.89      0.89      0.89     16500\n",
      "\n",
      "\n",
      "\n",
      "Accuracy for C = 0.01: 0.8859393939393939\n",
      "\n",
      "\n",
      "Confusion matric for for C = 0.05: \n",
      " [[7246  962]\n",
      " [ 804 7488]]\n",
      "\n",
      "\n",
      "Classification report for C = 0.05: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.88      0.89      8208\n",
      "    positive       0.89      0.90      0.89      8292\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     16500\n",
      "   macro avg       0.89      0.89      0.89     16500\n",
      "weighted avg       0.89      0.89      0.89     16500\n",
      "\n",
      "\n",
      "\n",
      "Accuracy for C = 0.05: 0.892969696969697\n",
      "\n",
      "\n",
      "Confusion matric for for C = 0.25: \n",
      " [[7244  964]\n",
      " [ 841 7451]]\n",
      "\n",
      "\n",
      "Classification report for C = 0.25: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.88      0.89      8208\n",
      "    positive       0.89      0.90      0.89      8292\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     16500\n",
      "   macro avg       0.89      0.89      0.89     16500\n",
      "weighted avg       0.89      0.89      0.89     16500\n",
      "\n",
      "\n",
      "\n",
      "Accuracy for C = 0.25: 0.8906060606060606\n",
      "\n",
      "\n",
      "Confusion matric for for C = 0.5: \n",
      " [[7219  989]\n",
      " [ 862 7430]]\n",
      "\n",
      "\n",
      "Classification report for C = 0.5: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.88      0.89      8208\n",
      "    positive       0.88      0.90      0.89      8292\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     16500\n",
      "   macro avg       0.89      0.89      0.89     16500\n",
      "weighted avg       0.89      0.89      0.89     16500\n",
      "\n",
      "\n",
      "\n",
      "Accuracy for C = 0.5: 0.8878181818181818\n",
      "\n",
      "\n",
      "Confusion matric for for C = 1: \n",
      " [[7200 1008]\n",
      " [ 898 7394]]\n",
      "\n",
      "\n",
      "Classification report for C = 1: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.88      0.88      8208\n",
      "    positive       0.88      0.89      0.89      8292\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     16500\n",
      "   macro avg       0.88      0.88      0.88     16500\n",
      "weighted avg       0.88      0.88      0.88     16500\n",
      "\n",
      "\n",
      "\n",
      "Accuracy for C = 1: 0.8844848484848484\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train1, y_train)\n",
    "    print (f\"Confusion matric for for C = {c}: \\n {metrics.confusion_matrix(y_test, lr.predict(X_test1))}\")\n",
    "    print('\\n')\n",
    "    print (f\"Classification report for C = {c}: \\n {metrics.classification_report(y_test, lr.predict(X_test1))}\")\n",
    "    print('\\n')\n",
    "    print (f\"Accuracy for C = {c}: {metrics.accuracy_score(y_test, lr.predict(X_test1))}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing every review and performing analysis\n",
    "\n",
    "### Function to lemmatize sentences in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    \n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    \n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            \n",
    "            lemmatized_sentence.append(word)\n",
    "        else:\n",
    "            \n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LemmatizedReviews'] = df['reviews'].apply(lambda x: lemmatize_sentence(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing analysis on lemmatized reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = df['LemmatizedReviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train, y_test = train_test_split(X1, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7230  978]\n",
      " [1407 6885]]\n"
     ]
    }
   ],
   "source": [
    "text_clf_nb.fit(X_train2, y_train)\n",
    "pred = text_clf_nb.predict(X_test2)\n",
    "print(metrics.confusion_matrix(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.88      0.86      8208\n",
      "    positive       0.88      0.83      0.85      8292\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     16500\n",
      "   macro avg       0.86      0.86      0.86     16500\n",
      "weighted avg       0.86      0.86      0.86     16500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8554545454545455\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7274  934]\n",
      " [ 763 7529]]\n"
     ]
    }
   ],
   "source": [
    "text_clf_lsvc.fit(X_train2, y_train)\n",
    "pred1 = text_clf_lsvc.predict(X_test2)\n",
    "print(metrics.confusion_matrix(y_test,pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.89      0.90      8208\n",
      "    positive       0.89      0.91      0.90      8292\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     16500\n",
      "   macro avg       0.90      0.90      0.90     16500\n",
      "weighted avg       0.90      0.90      0.90     16500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8971515151515151\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test,pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying stopwords after Lemmatization and performing same analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7209  999]\n",
      " [1341 6951]]\n"
     ]
    }
   ],
   "source": [
    "text_clf_nb2.fit(X_train2, y_train)\n",
    "pred2 = text_clf_nb2.predict(X_test2)\n",
    "print(metrics.confusion_matrix(y_test,pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.88      0.86      8208\n",
      "    positive       0.87      0.84      0.86      8292\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     16500\n",
      "   macro avg       0.86      0.86      0.86     16500\n",
      "weighted avg       0.86      0.86      0.86     16500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8581818181818182\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test,pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7260  948]\n",
      " [ 794 7498]]\n"
     ]
    }
   ],
   "source": [
    "text_clf_lsvc2.fit(X_train2, y_train)\n",
    "pred3 = text_clf_lsvc2.predict(X_test2)\n",
    "print(metrics.confusion_matrix(y_test,pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.88      0.89      8208\n",
      "    positive       0.89      0.90      0.90      8292\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     16500\n",
      "   macro avg       0.89      0.89      0.89     16500\n",
      "weighted avg       0.89      0.89      0.89     16500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8944242424242425\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test,pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(X_train)\n",
    "X_train3 = cv.transform(X_train2)\n",
    "X_test3 = cv.transform(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matric for for C = 0.01: \n",
      " [[7144 1064]\n",
      " [ 942 7350]]\n",
      "\n",
      "\n",
      "Classification report for C = 0.01: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.87      0.88      8208\n",
      "    positive       0.87      0.89      0.88      8292\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     16500\n",
      "   macro avg       0.88      0.88      0.88     16500\n",
      "weighted avg       0.88      0.88      0.88     16500\n",
      "\n",
      "\n",
      "\n",
      "Accuracy for C = 0.01: 0.8784242424242424\n",
      "\n",
      "\n",
      "Confusion matric for for C = 0.05: \n",
      " [[7215  993]\n",
      " [ 878 7414]]\n",
      "\n",
      "\n",
      "Classification report for C = 0.05: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.88      0.89      8208\n",
      "    positive       0.88      0.89      0.89      8292\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     16500\n",
      "   macro avg       0.89      0.89      0.89     16500\n",
      "weighted avg       0.89      0.89      0.89     16500\n",
      "\n",
      "\n",
      "\n",
      "Accuracy for C = 0.05: 0.8866060606060606\n",
      "\n",
      "\n",
      "Confusion matric for for C = 0.25: \n",
      " [[7227  981]\n",
      " [ 878 7414]]\n",
      "\n",
      "\n",
      "Classification report for C = 0.25: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.88      0.89      8208\n",
      "    positive       0.88      0.89      0.89      8292\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     16500\n",
      "   macro avg       0.89      0.89      0.89     16500\n",
      "weighted avg       0.89      0.89      0.89     16500\n",
      "\n",
      "\n",
      "\n",
      "Accuracy for C = 0.25: 0.8873333333333333\n",
      "\n",
      "\n",
      "Confusion matric for for C = 0.5: \n",
      " [[7209  999]\n",
      " [ 915 7377]]\n",
      "\n",
      "\n",
      "Classification report for C = 0.5: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.88      0.88      8208\n",
      "    positive       0.88      0.89      0.89      8292\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     16500\n",
      "   macro avg       0.88      0.88      0.88     16500\n",
      "weighted avg       0.88      0.88      0.88     16500\n",
      "\n",
      "\n",
      "\n",
      "Accuracy for C = 0.5: 0.884\n",
      "\n",
      "\n",
      "Confusion matric for for C = 1: \n",
      " [[7176 1032]\n",
      " [ 944 7348]]\n",
      "\n",
      "\n",
      "Classification report for C = 1: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.87      0.88      8208\n",
      "    positive       0.88      0.89      0.88      8292\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     16500\n",
      "   macro avg       0.88      0.88      0.88     16500\n",
      "weighted avg       0.88      0.88      0.88     16500\n",
      "\n",
      "\n",
      "\n",
      "Accuracy for C = 1: 0.8802424242424243\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train3, y_train)\n",
    "    print (f\"Confusion matric for for C = {c}: \\n {metrics.confusion_matrix(y_test, lr.predict(X_test3))}\")\n",
    "    print('\\n')\n",
    "    print (f\"Classification report for C = {c}: \\n {metrics.classification_report(y_test, lr.predict(X_test3))}\")\n",
    "    print('\\n')\n",
    "    print (f\"Accuracy for C = {c}: {metrics.accuracy_score(y_test, lr.predict(X_test3))}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming using PorterStemmer and Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import PorterStemmer, SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stemmer1 = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_sentences(sentence):\n",
    "    tokens = sentence.split()\n",
    "    stemmed_tokens = [stemmer1.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['porterReviews'] = df['reviews'].apply(stem_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem1_sentences(sentence):\n",
    "    tokens = sentence.split()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['snowballReviews'] = df['reviews'].apply(stem1_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = df['porterReviews']\n",
    "X3 = df['snowballReviews']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing same analysis of both models on reviews obtained after Porter stemming and Snowball stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train3, X_test3, y_train, y_test = train_test_split(X2, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train4, X_test4, y_train, y_test = train_test_split(X3, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7255  953]\n",
      " [1400 6892]]\n"
     ]
    }
   ],
   "source": [
    "text_clf_nb.fit(X_train3, y_train)\n",
    "predict1 = text_clf_nb.predict(X_test3)\n",
    "print(metrics.confusion_matrix(y_test,predict1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.88      0.86      8208\n",
      "    positive       0.88      0.83      0.85      8292\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     16500\n",
      "   macro avg       0.86      0.86      0.86     16500\n",
      "weighted avg       0.86      0.86      0.86     16500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,predict1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8573939393939394\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test,predict1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7270  938]\n",
      " [ 751 7541]]\n"
     ]
    }
   ],
   "source": [
    "text_clf_lsvc.fit(X_train3, y_train)\n",
    "predict2 = text_clf_lsvc.predict(X_test3)\n",
    "print(metrics.confusion_matrix(y_test,predict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.89      0.90      8208\n",
      "    positive       0.89      0.91      0.90      8292\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     16500\n",
      "   macro avg       0.90      0.90      0.90     16500\n",
      "weighted avg       0.90      0.90      0.90     16500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,predict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8976363636363637\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test,predict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7245  963]\n",
      " [1417 6875]]\n"
     ]
    }
   ],
   "source": [
    "text_clf_nb.fit(X_train4, y_train)\n",
    "predict3 = text_clf_nb.predict(X_test4)\n",
    "print(metrics.confusion_matrix(y_test,predict3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.88      0.86      8208\n",
      "    positive       0.88      0.83      0.85      8292\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     16500\n",
      "   macro avg       0.86      0.86      0.86     16500\n",
      "weighted avg       0.86      0.86      0.86     16500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,predict3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8557575757575757\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test,predict3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7256  952]\n",
      " [ 765 7527]]\n"
     ]
    }
   ],
   "source": [
    "text_clf_lsvc.fit(X_train4, y_train)\n",
    "predict4 = text_clf_lsvc.predict(X_test4)\n",
    "print(metrics.confusion_matrix(y_test,predict4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.88      0.89      8208\n",
      "    positive       0.89      0.91      0.90      8292\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     16500\n",
      "   macro avg       0.90      0.90      0.90     16500\n",
      "weighted avg       0.90      0.90      0.90     16500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,predict4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8959393939393939\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test,predict4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using VADER to perform analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['scores'] = df['reviews'].apply(lambda review: sid.polarity_scores(review))\n",
    "df['compound']  = df['scores'].apply(lambda score_dict: score_dict['compound'])\n",
    "df['comp_score'] = df['compound'].apply(lambda c: 'positive' if c >=0 else 'negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>reviews</th>\n",
       "      <th>LemmatizedReviews</th>\n",
       "      <th>porterReviews</th>\n",
       "      <th>snowballReviews</th>\n",
       "      <th>scores</th>\n",
       "      <th>compound</th>\n",
       "      <th>comp_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>one of the other reviewer have mention that af...</td>\n",
       "      <td>one of the other review ha mention that after ...</td>\n",
       "      <td>one of the other review has mention that after...</td>\n",
       "      <td>{'neg': 0.198, 'neu': 0.746, 'pos': 0.057, 'co...</td>\n",
       "      <td>-0.9947</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>a wonderful little production.  the filming te...</td>\n",
       "      <td>a wonderful little production . the filming te...</td>\n",
       "      <td>a wonder littl production. the film techniqu i...</td>\n",
       "      <td>a wonder littl production. the film techniqu i...</td>\n",
       "      <td>{'neg': 0.054, 'neu': 0.771, 'pos': 0.175, 'co...</td>\n",
       "      <td>0.9641</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>i think this be a wonderful way to spend time ...</td>\n",
       "      <td>i thought thi wa a wonder way to spend time on...</td>\n",
       "      <td>i thought this was a wonder way to spend time ...</td>\n",
       "      <td>{'neg': 0.092, 'neu': 0.69, 'pos': 0.217, 'com...</td>\n",
       "      <td>0.9780</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>basically there 's a family where a little boy...</td>\n",
       "      <td>basic there' a famili where a littl boy (jake)...</td>\n",
       "      <td>basic there a famili where a littl boy (jake) ...</td>\n",
       "      <td>{'neg': 0.141, 'neu': 0.777, 'pos': 0.083, 'co...</td>\n",
       "      <td>-0.8996</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>petter mattei 's `` love in the time of money ...</td>\n",
       "      <td>petter mattei' \"love in the time of money\" is ...</td>\n",
       "      <td>petter mattei \"love in the time of money\" is a...</td>\n",
       "      <td>{'neg': 0.052, 'neu': 0.791, 'pos': 0.157, 'co...</td>\n",
       "      <td>0.9766</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                            reviews  \\\n",
       "0  positive  one of the other reviewers has mentioned that ...   \n",
       "1  positive  a wonderful little production.  the filming te...   \n",
       "2  positive  i thought this was a wonderful way to spend ti...   \n",
       "3  negative  basically there's a family where a little boy ...   \n",
       "4  positive  petter mattei's \"love in the time of money\" is...   \n",
       "\n",
       "                                   LemmatizedReviews  \\\n",
       "0  one of the other reviewer have mention that af...   \n",
       "1  a wonderful little production . the filming te...   \n",
       "2  i think this be a wonderful way to spend time ...   \n",
       "3  basically there 's a family where a little boy...   \n",
       "4  petter mattei 's `` love in the time of money ...   \n",
       "\n",
       "                                       porterReviews  \\\n",
       "0  one of the other review ha mention that after ...   \n",
       "1  a wonder littl production. the film techniqu i...   \n",
       "2  i thought thi wa a wonder way to spend time on...   \n",
       "3  basic there' a famili where a littl boy (jake)...   \n",
       "4  petter mattei' \"love in the time of money\" is ...   \n",
       "\n",
       "                                     snowballReviews  \\\n",
       "0  one of the other review has mention that after...   \n",
       "1  a wonder littl production. the film techniqu i...   \n",
       "2  i thought this was a wonder way to spend time ...   \n",
       "3  basic there a famili where a littl boy (jake) ...   \n",
       "4  petter mattei \"love in the time of money\" is a...   \n",
       "\n",
       "                                              scores  compound comp_score  \n",
       "0  {'neg': 0.198, 'neu': 0.746, 'pos': 0.057, 'co...   -0.9947   negative  \n",
       "1  {'neg': 0.054, 'neu': 0.771, 'pos': 0.175, 'co...    0.9641   positive  \n",
       "2  {'neg': 0.092, 'neu': 0.69, 'pos': 0.217, 'com...    0.9780   positive  \n",
       "3  {'neg': 0.141, 'neu': 0.777, 'pos': 0.083, 'co...   -0.8996   negative  \n",
       "4  {'neg': 0.052, 'neu': 0.791, 'pos': 0.157, 'co...    0.9766   positive  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13507 11493]\n",
      " [ 3624 21376]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(df['sentiment'],df['comp_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.54      0.64     25000\n",
      "    positive       0.65      0.86      0.74     25000\n",
      "\n",
      "   micro avg       0.70      0.70      0.70     50000\n",
      "   macro avg       0.72      0.70      0.69     50000\n",
      "weighted avg       0.72      0.70      0.69     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(df['sentiment'],df['comp_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69766"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(df['sentiment'],df['comp_score'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
